{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:The testing module requires faiss. You can install the GPU version with the command 'conda install faiss-gpu -c pytorch' \n",
      "                        or the CPU version with 'conda install faiss-cpu -c pytorch'. Learn more at https://github.com/facebookresearch/faiss/blob/master/INSTALL.md\n"
     ]
    }
   ],
   "source": [
    "# DF-Net Imports\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from itertools import islice\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "# Imports\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pytorch_metric_learning import losses, samplers\n",
    "from random import randint\n",
    "from skimage import io, transform\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, models, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def map_features(outputs, labels, out_file):\n",
    "    # create array of column for each feature output\n",
    "    feat_cols = ['feature'+str(i) for i in range(outputs.shape[1])]\n",
    "    # make dataframe of outputs -> labels\n",
    "    df = pd.DataFrame(outputs, columns=feat_cols)\n",
    "    df['y'] = labels\n",
    "    df['labels'] = df['y'].apply(lambda i: str(i))\n",
    "    # clear outputs and labels\n",
    "    outputs, labels = None, None\n",
    "    # creates an array of random indices from size of outputs\n",
    "    np.random.seed(42)\n",
    "    rndperm = np.random.permutation(df.shape[0])\n",
    "    num_examples = 3000\n",
    "    df_subset = df.loc[rndperm[:num_examples],:].copy()\n",
    "    data_subset = df_subset[feat_cols].values\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(data_subset)\n",
    "    df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "    df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.scatter(\n",
    "        x=df_subset[\"tsne-2d-one\"],\n",
    "        y=df_subset[\"tsne-2d-two\"],\n",
    "        c=df_subset[\"y\"],\n",
    "        s=3\n",
    "    )\n",
    "    plt.savefig(out_file, bbox_inches='tight', pad_inches = 0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Num epochs\n",
    "num_epochs = 25\n",
    "\n",
    "# Model \n",
    "model = models.resnet50()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Data set\n",
    "train_path = '/lab/vislab/DATA/CUB/images/'\n",
    "\n",
    "# Loss function\n",
    "criterion = losses.TripletMarginLoss(margin=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomMask(object):\n",
    "    \"\"\"Add random occlusions to image.\n",
    "\n",
    "    Args:\n",
    "        mask: (Image.Image) - Image to use to occlude.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask):\n",
    "        assert isinstance(mask, Image.Image)\n",
    "        self.mask = mask\n",
    "\n",
    "    def __call__(self, sample):                    \n",
    "        self.mask = self.mask.resize((64,64))\n",
    "        theta = randint(0,45)\n",
    "        self.mask = self.mask.rotate(angle=theta)\n",
    "        seed_x = randint(1,10)\n",
    "        seed_y = randint(1,10)\n",
    "        sample.paste(self.mask, (20*seed_x, 20*seed_y))\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/lab/vislab/DATA/masks/mask.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d67f07c21ee9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# occlusion transform?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/lab/vislab/DATA/masks/mask.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m transformations = transforms.Compose([\n",
      "\u001b[0;32m/usr/lib64/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2764\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2765\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2766\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2767\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/lab/vislab/DATA/masks/mask.png'"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "\n",
    "# occlusion transform?\n",
    "mask = Image.open('/lab/vislab/DATA/masks/mask.png')\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    RandomMask(mask), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(train_path, transformations)\n",
    "\n",
    "plt.imshow(transforms.ToPILImage()(dataset[3][0]), interpolation=\"bicubic\")\n",
    "\n",
    "train_set, test_set = torch.utils.data.random_split(dataset, [5000, 1033])\n",
    "\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "                                   sampler=train_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net Utils\n",
    "\n",
    "def resize_like(x, target, mode='bilinear'):\n",
    "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
    "\n",
    "\n",
    "def list2nparray(lst, dtype=None):\n",
    "    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n",
    "    if isinstance(lst, np.ndarray):\n",
    "        return lst\n",
    "    assert isinstance(lst, (list, tuple)), 'bad type: {}'.format(type(lst))\n",
    "    assert lst, 'attempt to convert empty list to np array'\n",
    "    if isinstance(lst[0], np.ndarray):\n",
    "        dim1 = lst[0].shape\n",
    "        assert all(i.shape == dim1 for i in lst)\n",
    "        if dtype is None:\n",
    "            dtype = lst[0].dtype\n",
    "            assert all(i.dtype == dtype for i in lst), \\\n",
    "                'bad dtype: {} {}'.format(dtype, set(i.dtype for i in lst))\n",
    "    elif isinstance(lst[0], (int, float, complex, np.number)):\n",
    "        return np.array(lst, dtype=dtype)\n",
    "    else:\n",
    "        dim1 = list2nparray(lst[0])\n",
    "        if dtype is None:\n",
    "            dtype = dim1.dtype\n",
    "        dim1 = dim1.shape\n",
    "    shape = [len(lst)] + list(dim1)\n",
    "    rst = np.empty(shape, dtype=dtype)\n",
    "    for idx, i in enumerate(lst):\n",
    "        rst[idx] = i\n",
    "    return rst\n",
    "\n",
    "\n",
    "def get_img_list(path):\n",
    "    return sorted(list(Path(path).glob('*.png'))) + \\\n",
    "        sorted(list(Path(path).glob('*.jpg'))) + \\\n",
    "        sorted(list(Path(path).glob('*.jpeg')))\n",
    "\n",
    "\n",
    "def gen_miss(img, mask, output):\n",
    "\n",
    "    imgs = get_img_list(img)\n",
    "    masks = get_img_list(mask)\n",
    "    print('Total images:', len(imgs), len(masks))\n",
    "\n",
    "    out = Path(output)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, (img, mask) in tqdm(enumerate(zip(imgs, masks))):\n",
    "        path = out.joinpath('miss_%04d.png' % (i+1))\n",
    "        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(str(mask), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, img.shape[:2][::-1])\n",
    "        mask = mask[..., np.newaxis]\n",
    "        miss = img * (mask > 127) + 255 * (mask <= 127)\n",
    "        cv2.imwrite(str(path), miss)\n",
    "\n",
    "def merge_imgs(dirs, output, row=1, gap=2, res=512):\n",
    "\n",
    "    image_list = [get_img_list(path) for path in dirs]\n",
    "    img_count = [len(image) for image in image_list]\n",
    "    print('Total images:', img_count) # should be batch size when we walk back into here ... \n",
    "    assert min(img_count) > 0, 'Please check the path of empty folder.'\n",
    "\n",
    "    output_dir = Path(output)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_img = len(dirs)\n",
    "    row = row\n",
    "    column = (n_img - 1) // row + 1\n",
    "    print('Row:', row)\n",
    "    print('Column:', column)\n",
    "\n",
    "    for i, unit in tqdm(enumerate(zip(*image_list))):\n",
    "        name = output_dir.joinpath('merge_%04d.png' % i)\n",
    "        merge = np.ones([\n",
    "            res*row + (row+1)*gap, res*column + (column+1)*gap, 3], np.uint8) * 255\n",
    "        for j, img in enumerate(unit):\n",
    "            r = j // column\n",
    "            c = j - r * column\n",
    "            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
    "            if img.shape[:2] != (res, res):\n",
    "                img = cv2.resize(img, (res, res))\n",
    "            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n",
    "            merge[start_h: start_h + res, start_w: start_w + res] = img\n",
    "        cv2.imwrite(str(name), merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net Model\n",
    "\n",
    "def get_norm(name, out_channels):\n",
    "    if name == 'batch':\n",
    "        norm = nn.BatchNorm2d(out_channels)\n",
    "    elif name == 'instance':\n",
    "        norm = nn.InstanceNorm2d(out_channels)\n",
    "    else:\n",
    "        norm = None\n",
    "    return norm\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == 'relu':\n",
    "        activation = nn.ReLU()\n",
    "    elif name == 'elu':\n",
    "        activation == nn.ELU()\n",
    "    elif name == 'leaky_relu':\n",
    "        activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "    elif name == 'tanh':\n",
    "        activation = nn.Tanh()\n",
    "    elif name == 'sigmoid':\n",
    "        activation = nn.Sigmoid()\n",
    "    else:\n",
    "        activation = None\n",
    "    return activation\n",
    "\n",
    "\n",
    "class Conv2dSame(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = self.conv_same_pad(kernel_size, stride)\n",
    "        if type(padding) is not tuple:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding)\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConstantPad2d(padding*2, 0),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
    "            )\n",
    "\n",
    "    def conv_same_pad(self, ksize, stride):\n",
    "        if (ksize - stride) % 2 == 0:\n",
    "            return (ksize - stride) // 2\n",
    "        else:\n",
    "            left = (ksize - stride) // 2\n",
    "            right = left + 1\n",
    "            return left, right\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvTranspose2dSame(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n",
    "        self.trans_conv = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size, stride,\n",
    "            padding, output_padding)\n",
    "\n",
    "    def deconv_same_pad(self, ksize, stride):\n",
    "        pad = (ksize - stride + 1) // 2\n",
    "        outpad = 2 * pad + stride - ksize\n",
    "        return pad, outpad\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trans_conv(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        if mode == 'deconv':\n",
    "            self.up = ConvTranspose2dSame(\n",
    "                channel, channel, kernel_size, stride=scale)\n",
    "        else:\n",
    "            def upsample(x):\n",
    "                return F.interpolate(x, scale_factor=scale, mode=mode)\n",
    "            self.up = upsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class EncodeBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride,\n",
    "            normalization=None, activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_in = in_channels\n",
    "        self.c_out = out_channels\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            Conv2dSame(self.c_in, self.c_out, kernel_size, stride))\n",
    "        if normalization:\n",
    "            layers.append(get_norm(normalization, self.c_out))\n",
    "        if activation:\n",
    "            layers.append(get_activation(activation))\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class DecodeBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, c_from_up, c_from_down, c_out, mode='nearest',\n",
    "            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_from_up = c_from_up\n",
    "        self.c_from_down = c_from_down\n",
    "        self.c_in = c_from_up + c_from_down\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            Conv2dSame(self.c_in, self.c_out, kernel_size, stride=1))\n",
    "        if normalization:\n",
    "            layers.append(get_norm(normalization, self.c_out))\n",
    "        if activation:\n",
    "            layers.append(get_activation(activation))\n",
    "        self.decode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, concat=None):\n",
    "        out = self.up(x)\n",
    "        if self.c_from_down > 0:\n",
    "            out = torch.cat([out, concat], dim=1)\n",
    "        out = self.decode(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BlendBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, c_in, c_out, ksize_mid=3, norm='batch', act='leaky_relu'):\n",
    "        super().__init__()\n",
    "        c_mid = max(c_in // 2, 32)\n",
    "        self.blend = nn.Sequential(\n",
    "            Conv2dSame(c_in, c_mid, 1, 1),\n",
    "            get_norm(norm, c_mid),\n",
    "            get_activation(act),\n",
    "            Conv2dSame(c_mid, c_out, ksize_mid, 1),\n",
    "            get_norm(norm, c_out),\n",
    "            get_activation(act),\n",
    "            Conv2dSame(c_out, c_out, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blend(x)\n",
    "\n",
    "\n",
    "class FusionBlock(nn.Module):\n",
    "    def __init__(self, c_feat, c_alpha=1):\n",
    "        super().__init__()\n",
    "        c_img = 3\n",
    "        self.map2img = nn.Sequential(\n",
    "            Conv2dSame(c_feat, c_img, 1, 1),\n",
    "            nn.Sigmoid())\n",
    "        self.blend = BlendBlock(c_img*2, c_alpha)\n",
    "\n",
    "    def forward(self, img_miss, feat_de):\n",
    "        img_miss = resize_like(img_miss, feat_de)\n",
    "        raw = self.map2img(feat_de)\n",
    "        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n",
    "        result = alpha * raw + (1 - alpha) * img_miss\n",
    "        return result, alpha, raw\n",
    "\n",
    "\n",
    "class DFNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, c_img=3, c_mask=1, c_alpha=3,\n",
    "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
    "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n",
    "            blend_layers=[0, 1, 2, 3, 4, 5]):\n",
    "        super().__init__()\n",
    "\n",
    "        c_init = c_img + c_mask\n",
    "\n",
    "        self.n_en = len(en_ksize)\n",
    "        self.n_de = len(de_ksize)\n",
    "        assert self.n_en == self.n_de, (\n",
    "            'The number layer of Encoder and Decoder must be equal.')\n",
    "        assert self.n_en >= 1, (\n",
    "            'The number layer of Encoder and Decoder must be greater than 1.')\n",
    "\n",
    "        assert 0 in blend_layers, 'Layer 0 must be blended.'\n",
    "\n",
    "        self.en = []\n",
    "        c_in = c_init\n",
    "        self.en.append(\n",
    "            EncodeBlock(4, 64, en_ksize[0], 2, None, None)) # changing 4 breaks it\n",
    "        for k_en in en_ksize[1:]:\n",
    "            c_in = self.en[-1].c_out\n",
    "            c_out = min(c_in*2, 512)\n",
    "            self.en.append(EncodeBlock(\n",
    "                c_in, c_out, k_en, stride=2,\n",
    "                normalization=norm, activation=act_en))\n",
    "\n",
    "        # register parameters\n",
    "        for i, en in enumerate(self.en):\n",
    "            self.__setattr__('en_{}'.format(i), en)\n",
    "\n",
    "        self.de = []\n",
    "        self.fuse = []\n",
    "        for i, k_de in enumerate(de_ksize):\n",
    "\n",
    "            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n",
    "            c_out = c_from_down = self.en[-i-1].c_in\n",
    "            layer_idx = self.n_de - i - 1\n",
    "\n",
    "            self.de.append(DecodeBlock(\n",
    "                c_from_up, c_from_down, c_out, mode, k_de, scale=2,\n",
    "                normalization=norm, activation=act_de))\n",
    "            if layer_idx in blend_layers:\n",
    "                self.fuse.append(FusionBlock(c_out, c_alpha))\n",
    "            else:\n",
    "                self.fuse.append(None)\n",
    "\n",
    "        # register parameters\n",
    "        for i, de in enumerate(self.de[::-1]):\n",
    "            self.__setattr__('de_{}'.format(i), de)\n",
    "        for i, fuse in enumerate(self.fuse[::-1]):\n",
    "            if fuse:\n",
    "                self.__setattr__('fuse_{}'.format(i), fuse)\n",
    "\n",
    "#     def forward(self, img_miss, mask):\n",
    "    def forward(self, img_miss):\n",
    "\n",
    "#         out = torch.cat([img_miss, mask], dim=1)\n",
    "\n",
    "        # because our out will already have the mask on it\n",
    "        out = img_miss\n",
    "        \n",
    "        out_en = [out]\n",
    "        for encode in self.en:\n",
    "            out = encode(out)\n",
    "            out_en.append(out)\n",
    "\n",
    "        results = []\n",
    "        alphas = []\n",
    "        raws = []\n",
    "        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n",
    "            out = decode(out, out_en[-i-2])\n",
    "            if fuse:\n",
    "                result, alpha, raw = fuse(out, out)\n",
    "                results.append(result)\n",
    "                alphas.append(alpha)\n",
    "                raws.append(raw)\n",
    "\n",
    "        return results[::-1], alphas[::-1], raws[::-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net inpaint (from the DF-Net tester)\n",
    "\n",
    "class Inpainter:\n",
    "\n",
    "    def __init__(self, model_path, input_size, batch_size):\n",
    "        self.model_path = model_path\n",
    "        self._input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.init_model(model_path)\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        if self._input_size > 0:\n",
    "            return (self._input_size, self._input_size)\n",
    "        elif 'celeba' in self.model_path:\n",
    "            return (256, 256)\n",
    "        else:\n",
    "            return (512, 512)\n",
    "\n",
    "    def init_model(self, path):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print('Using gpu.')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print('Using cpu.')\n",
    "\n",
    "        self.model1 = DFNet().to(self.device)\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model1.load_state_dict(checkpoint)\n",
    "        self.model1.eval()\n",
    "\n",
    "        print('Model1 %s loaded.' % path)\n",
    "\n",
    "    def get_name(self, path):\n",
    "        return '.'.join(path.name.split('.')[:-1])\n",
    "\n",
    "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
    "        img_name = self.get_name(img_path)\n",
    "        mask_name = self.get_name(mask_path)\n",
    "        return {\n",
    "            'result_path': self.sub_dir('result').joinpath(\n",
    "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
    "            'raw_path': self.sub_dir('raw').joinpath(\n",
    "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
    "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
    "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
    "        }\n",
    "\n",
    "    def inpaint_instance(self, img, mask):\n",
    "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
    "        img = img.view(1, *img.shape)\n",
    "        mask = mask.view(1, 1, *mask.shape)\n",
    "        return self.inpaint_batch(img, mask).squeeze()\n",
    "\n",
    "    def inpaint_batch(self, imgs, masks):\n",
    "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
    "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
    "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
    "\n",
    "        imgs = torch.from_numpy(imgs).to(self.device)\n",
    "        masks = torch.from_numpy(masks).to(self.device)\n",
    "        imgs = imgs.float().div(255)\n",
    "        masks = masks.float().div(255)\n",
    "        imgs_miss = imgs * masks\n",
    "        results = self.model1(imgs_miss, masks)\n",
    "        if type(results) is list:\n",
    "            results = results[0]\n",
    "        results = results.mul(255).byte().data.cpu().numpy()\n",
    "        results = np.transpose(results, [0, 2, 3, 1])\n",
    "        return results\n",
    "\n",
    "    def _process_file(self, output, img_path, mask_path):\n",
    "        item = {\n",
    "            'img_path': img_path,\n",
    "            'mask_path': mask_path,\n",
    "        }\n",
    "        item.update(self.results_path(output, img_path, mask_path))\n",
    "        self.path_pair.append(item)\n",
    "\n",
    "    def process_single_file(self, output, img_path, mask_path):\n",
    "        self.path_pair = []\n",
    "        self._process_file(output, img_path, mask_path)\n",
    "\n",
    "    def process_dir(self, output, img_dir, mask_dir):\n",
    "        img_dir = Path(img_dir)\n",
    "        mask_dir = Path(mask_dir)\n",
    "        imgs_path = sorted(\n",
    "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
    "        masks_path = sorted(\n",
    "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
    "\n",
    "        n_img = len(imgs_path)\n",
    "        n_mask = len(masks_path)\n",
    "        n_pair = min(n_img, n_mask)\n",
    "\n",
    "        self.path_pair = []\n",
    "        for i in range(n_pair):\n",
    "            img_path = imgs_path[i % n_img]\n",
    "            mask_path = masks_path[i % n_mask]\n",
    "            self._process_file(output, img_path, mask_path)\n",
    "\n",
    "    def get_process(self, input_size):\n",
    "        def process(pair):\n",
    "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
    "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
    "            if input_size:\n",
    "                img = cv2.resize(img, input_size)\n",
    "                mask = cv2.resize(mask, input_size)\n",
    "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
    "            mask = np.ascontiguousarray(\n",
    "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
    "\n",
    "            pair['img'] = img\n",
    "            pair['mask'] = mask\n",
    "            return pair\n",
    "        return process\n",
    "\n",
    "    def _file_batch(self):\n",
    "        pool = Pool()\n",
    "\n",
    "        n_pair = len(self.path_pair)\n",
    "        n_batch = (n_pair-1) // self.batch_size + 1\n",
    "\n",
    "        for i in tqdm.trange(n_batch, leave=False):\n",
    "            _buffer = defaultdict(list)\n",
    "            start = i * self.batch_size\n",
    "            stop = start + self.batch_size\n",
    "            process = self.get_process(self.input_size)\n",
    "            batch = pool.imap_unordered(\n",
    "                process, islice(self.path_pair, start, stop))\n",
    "            for instance in batch:\n",
    "                for k, v in instance.items():\n",
    "                    _buffer[k].append(v)\n",
    "            yield _buffer\n",
    "\n",
    "#     def batch_generator(self):\n",
    "#         generator = self._file_batch\n",
    "\n",
    "#         for _buffer in generator():\n",
    "#             for key in _buffer:\n",
    "#                 if key in ['img', 'mask']:\n",
    "#                     _buffer[key] = list2nparray(_buffer[key])\n",
    "#             yield _buffer\n",
    "\n",
    "    def to_numpy(self, tensor):\n",
    "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
    "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
    "        return tensor\n",
    "\n",
    "    def process_batch(self, batch, output):\n",
    "#         imgs = torch.from_numpy(batch['img']).to(self.device)\n",
    "#         masks = torch.from_numpy(batch['mask']).to(self.device)\n",
    "#         imgs = imgs.float().div(255)\n",
    "#         masks = masks.float().div(255)\n",
    "\n",
    "        # so we wantt  change our tensors to numpy\n",
    "    \n",
    "    \n",
    "    #### what does this do? ####\n",
    "        # imgs_miss = imgs * masks\n",
    "        \n",
    "        ## occluded_imgs = batch[0...32] ##\n",
    "\n",
    "        ## So the model needs the separate mask ##\n",
    "        # result is the one we care about\n",
    "        \n",
    "        ## maybe the model also has to be init? ##\n",
    "        \n",
    "        print(batch.shape)\n",
    "        \n",
    "        time.sleep(10)\n",
    "        \n",
    "        result, alpha, raw = self.model1(batch)\n",
    "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
    "        result = imgs * masks + result * (1 - masks)\n",
    "\n",
    "        result = self.to_numpy(result)\n",
    "        result = torch.from_numpy(result)\n",
    "        \n",
    "        \n",
    "#         alpha = self.to_numpy(alpha)\n",
    "#         raw = self.to_numpy(raw)\n",
    "\n",
    "#         for i in range(batch.shape[0]):\n",
    "#             cv2.imwrite(str(batch['result_path'][i]))\n",
    "#         for i in range(result.shape[0]):\n",
    "#             cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
    "#             cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
    "#             cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
    "            \n",
    "        return result\n",
    "\n",
    "    @property\n",
    "    def root(self):\n",
    "        return Path(self.output)\n",
    "\n",
    "    def sub_dir(self, sub):\n",
    "        return self.root.joinpath(sub)\n",
    "\n",
    "    def prepare_folders(self, folders):\n",
    "        for folder in folders:\n",
    "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def inpaint(self, output, batch, merge_result=True):\n",
    "\n",
    "        # so we get passed in a batch with already occluded images\n",
    "        \n",
    "        self.output = output\n",
    "        self.prepare_folders([\n",
    "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
    "            self.sub_dir('raw')])\n",
    "\n",
    "        # okay, so img is a torch batch tensor now:\n",
    "        \n",
    "#         if os.path.isfile(img) and os.path.isfile(mask):\n",
    "#             if img.endswith(('.png', '.jpg', '.jpeg')):\n",
    "#                 self.process_single_file(output, img, mask)\n",
    "#                 _type = 'file'\n",
    "#             else:\n",
    "#                 raise NotImplementedError()\n",
    "#         elif os.path.isdir(img) and os.path.isdir(mask):\n",
    "#             self.process_dir(output, img, mask)\n",
    "#             _type = 'dir'\n",
    "#         else:\n",
    "#             print('Img: ', img)\n",
    "#             print('Mask: ', mask)\n",
    "#             raise NotImplementedError(\n",
    "#                 'img and mask should be both file or directory.')\n",
    "\n",
    "        print('# Inpainting...')\n",
    "        print('Input size:', self.input_size)\n",
    "        \n",
    "        modified_batch = []\n",
    "        for image in batch:\n",
    "            m = self.process_batch(image, output)\n",
    "            modified_batch.append(m)\n",
    "#         modified_batch = self.process_batch(batch, output)\n",
    "        print('Inpainting finished.')\n",
    "\n",
    "#         if merge_result and _type == 'dir':\n",
    "#             miss = self.sub_dir('miss')\n",
    "#             merge = self.sub_dir('merge')\n",
    "\n",
    "#             print('# Preparing input images...')\n",
    "#             gen_miss(img, mask, miss)\n",
    "#             print('# Merging...')\n",
    "#             merge_imgs([\n",
    "#                 miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
    "#                 self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
    "#             print('Merging finished.')\n",
    "            \n",
    "            \n",
    "        return modified_batch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Trainer\n",
    "def train_model():\n",
    "    \"\"\"Generic function to train model\"\"\"\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    correct = 0 \n",
    "    incorrect = 0 \n",
    "    num_batches = 0\n",
    "    loss_values = []\n",
    "\n",
    "    # DF-Net Tester Instantiate\n",
    "    pretrained_model_path = './model/model_places2.pth'\n",
    "    inpainter = Inpainter(pretrained_model_path, 256, 8)\n",
    "\n",
    "    # Epochs \n",
    "    for epoch in range(num_epochs): \n",
    "        print(\"epoch num:\", epoch)\n",
    "        \n",
    "        running_outputs = torch.FloatTensor().cpu()\n",
    "        running_labels = torch.LongTensor().cpu()\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "                                \n",
    "        # Batches\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader): \n",
    "            \n",
    "            # *** put the DF-Net call here on this call, NOT THE OTHER LOOP ***\n",
    "            \n",
    "            # they should already be masked\n",
    "            # inputs is a batch of 32 images, can access just like: inputs[31]\n",
    "            \n",
    "            inpainted_img_batch = inpainter.inpaint('output/', inputs, merge_result=True)\n",
    "            \n",
    "            num_batches += 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            inpainted_img_batch, labels = inpainted_img_batch.to(device), labels.to(device)\n",
    "            output = model.forward(inpainted_img_batch)\n",
    "            \n",
    "            running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "            running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        for idx, emb in enumerate(running_outputs.to(device)):    \n",
    "            pairwise = torch.nn.PairwiseDistance(p=2).to(device)[1]\n",
    "            dist = pairwise(emb, running_outputs.to(device))\n",
    "            closest = torch.topk(dist, 2, largest=False).indices[1]\n",
    "            if running_labels[idx] == running_labels[closest]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "\n",
    "        running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "        running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "\n",
    "        print(running_loss / num_batches)\n",
    "        print(\"Correct\", correct)\n",
    "        print(\"Incorrect\", incorrect)\n",
    "\n",
    "        # t-SNE\n",
    "        map_features(running_outputs, running_labels, \"outfile\")\n",
    "        # Loss Plot            \n",
    "        loss_values.append(running_loss / num_batches)\n",
    "\n",
    "        time_elapsed = datetime.now() - start_time \n",
    "        print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))    \n",
    "\n",
    "    plt.plot(loss_values)    \n",
    "    return model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test():\n",
    "    trained_model.eval()\n",
    "    test_running_outputs = torch.FloatTensor().cpu()\n",
    "    test_running_labels = torch.LongTensor().cpu()\n",
    "    test_running_loss = 0.0    \n",
    "    correct = 0 \n",
    "    incorrect = 0 \n",
    "    \n",
    "    test_sampler = torch.utils.data.RandomSampler(test_set)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \n",
    "                                           sampler=test_sampler, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            output = trained_model(data)\n",
    "            \n",
    "            test_running_outputs = torch.cat((test_running_outputs, output.cpu().detach()), 0)\n",
    "            test_running_labels = torch.cat((test_running_labels, labels.cpu().detach()), 0)\n",
    "    \n",
    "            test_loss = criterion(output, labels)\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        for idx, emb in enumerate(running_outputs.to(device)):    \n",
    "            pairwise = torch.nn.PairwiseDistance(p=2).to(device)\n",
    "            dist = pairwise(emb, running_outputs.to(device))\n",
    "            closest = torch.topk(dist, 2, largest=False).indices[1]\n",
    "            if running_labels[idx] == running_labels[closest]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                \n",
    "        map_features(test_running_outputs, test_running_labels, \"test_outfile\")        \n",
    "        print(\"correct\", correct)\n",
    "        print(\"incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug cell\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "                                       sampler=train_sampler, num_workers=4)\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader): \n",
    "    print(batch_idx)\n",
    "    print(type(inputs))\n",
    "    # plot\n",
    "#     plt.imshow(transforms.ToPILImage()(inputs[31]), interpolation=\"bicubic\")\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8f26edc6747e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# perhaps parameterize the train model to take in our pre-processed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Run Script\n",
    "model.to(device)\n",
    "\n",
    "# perhaps parameterize the train model to take in our pre-processed data\n",
    "\n",
    "trained_model, loss = train_model()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Script\n",
    "\n",
    "print(type(trained_model))\n",
    "trained_model.test()\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DF-Net Train Loop \n",
    "# def train_dfnet():\n",
    "#     \"\"\"Generic function to train model\"\"\"\n",
    "    \n",
    "#     print(\"Training model ...\")\n",
    "\n",
    "#     start_time = datetime.now()\n",
    "#     loss_values = []\n",
    "#     num_batches = 0 \n",
    "    \n",
    "#     # Epochs \n",
    "#     for epoch in range(num_epochs): \n",
    "#         print(\"epoch num:\", epoch)\n",
    "#         train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "#         train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "#                                                sampler=train_sampler, num_workers=4)\n",
    "       \n",
    "#         running_outputs = torch.FloatTensor().cpu()\n",
    "#         running_labels = torch.LongTensor().cpu()\n",
    "#         running_loss = 0.0\n",
    "#         dfnet_model.train()\n",
    "        \n",
    "#         # Batches\n",
    "#         for batch_idx, (inputs, labels) in enumerate(train_loader):  \n",
    "                        \n",
    "#             num_batches += 1\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             output = dfnet_model.forward(inputs)\n",
    "            \n",
    "#             running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "#             running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "\n",
    "#             loss = criterion(output, labels)\n",
    "#             loss = Variable(loss, requires_grad = True)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Loss Plot            \n",
    "#         loss_values.append(running_loss / num_batches)\n",
    "\n",
    "#         time_elapsed = datetime.now() - start_time \n",
    "#         print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))    \n",
    "\n",
    "#     plt.plot(loss_values)    \n",
    "#     return dfnet_model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DF-Net outputs\n",
    "# # TODO: implement this\n",
    "# # but how can we parameterize thi?\n",
    "\n",
    "# def inpaint(): \n",
    "#     running_outputs = torch.FloatTensor().cpu()\n",
    "#     running_labels = torch.LongTensor().cpu()\n",
    "#     running_loss = 0.0  \n",
    "    \n",
    "#     dfnet_sampler = torch.utils.data.RandomSampler(test_set)\n",
    "#     dfnet_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \n",
    "#                                            sampler=dfnet_sampler, num_workers=4)\n",
    "    \n",
    "#     with torch.no_grad():        \n",
    "#         for data, labels in dfnet_loader:\n",
    "#             data, labels = data.to(device), labels.to(device)\n",
    "#             output = model(data)\n",
    "            \n",
    "#             running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "#             running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "    \n",
    "#             loss = criterion(output, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "            \n",
    "    \n",
    "#     print(type(output))\n",
    "#     print(output.shape)\n",
    "#     print(type(output[0][0]))\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train DF-Net\n",
    "# dfnet_model.to(device)\n",
    "# dfnet_model, loss = train_dfnet()\n",
    "\n",
    "# print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
