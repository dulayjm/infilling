{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net Imports\n",
    "from collections import defaultdict\n",
    "import cv2\n",
    "from itertools import islice\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "# Imports\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pytorch_metric_learning import losses, samplers\n",
    "from random import randint\n",
    "import shutil\n",
    "from skimage import io, transform\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from torchvision import datasets, models, transforms\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def map_features(outputs, labels, out_file):\n",
    "    # create array of column for each feature output\n",
    "    feat_cols = ['feature'+str(i) for i in range(outputs.shape[1])]\n",
    "    # make dataframe of outputs -> labels\n",
    "    df = pd.DataFrame(outputs, columns=feat_cols)\n",
    "    df['y'] = labels\n",
    "    df['labels'] = df['y'].apply(lambda i: str(i))\n",
    "    # clear outputs and labels\n",
    "    outputs, labels = None, None\n",
    "    # creates an array of random indices from size of outputs\n",
    "    np.random.seed(42)\n",
    "    rndperm = np.random.permutation(df.shape[0])\n",
    "    num_examples = 3000\n",
    "    df_subset = df.loc[rndperm[:num_examples],:].copy()\n",
    "    data_subset = df_subset[feat_cols].values\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=300)\n",
    "    tsne_results = tsne.fit_transform(data_subset)\n",
    "    df_subset['tsne-2d-one'] = tsne_results[:,0]\n",
    "    df_subset['tsne-2d-two'] = tsne_results[:,1]\n",
    "    plt.figure(figsize=(16,10))\n",
    "    plt.scatter(\n",
    "        x=df_subset[\"tsne-2d-one\"],\n",
    "        y=df_subset[\"tsne-2d-two\"],\n",
    "        c=df_subset[\"y\"],\n",
    "        s=3\n",
    "    )\n",
    "    plt.savefig(out_file, bbox_inches='tight', pad_inches = 0)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "# Device\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Num epochs\n",
    "num_epochs = 2\n",
    "\n",
    "# Model \n",
    "model = models.resnet50()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Data set\n",
    "train_path = '/lab/vislab/DATA/CUB/images/'\n",
    "\n",
    "mask_path = './samples/places2/mask/'\n",
    "\n",
    "# Loss function\n",
    "criterion = losses.TripletMarginLoss(margin=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "\n",
    "# mask_ = Image.open('/lab/vislab/DATA/masks/mask.jpeg')\n",
    "\n",
    "transformations = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(train_path, transformations)\n",
    "\n",
    "# plt.imshow(transforms.ToPILImage()(dataset[3][0]), interpolation=\"bicubic\")\n",
    "\n",
    "# train_set, test_set = torch.utils.data.random_split(dataset, [5000, 1033])\n",
    "# train_set, test_set = torch.utils.data.random_split(dataset, [33, 6000])\n",
    "\n",
    "\n",
    "### remove the split, for now at least ... ###\n",
    "\n",
    "# train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_sampler = samplers.MPerClassSampler(dataset.targets, 3, len(dataset)) # maybe train set\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                   sampler=train_sampler, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net Utils\n",
    "\n",
    "def resize_like(x, target, mode='bilinear'):\n",
    "    return F.interpolate(x, target.shape[-2:], mode=mode, align_corners=False)\n",
    "\n",
    "\n",
    "def list2nparray(lst, dtype=None):\n",
    "    \"\"\"fast conversion from nested list to ndarray by pre-allocating space\"\"\"\n",
    "    if isinstance(lst, np.ndarray):\n",
    "        return lst\n",
    "    assert isinstance(lst, (list, tuple)), 'bad type: {}'.format(type(lst))\n",
    "    assert lst, 'attempt to convert empty list to np array'\n",
    "    if isinstance(lst[0], np.ndarray):\n",
    "        dim1 = lst[0].shape\n",
    "        assert all(i.shape == dim1 for i in lst)\n",
    "        if dtype is None:\n",
    "            dtype = lst[0].dtype\n",
    "            assert all(i.dtype == dtype for i in lst), \\\n",
    "                'bad dtype: {} {}'.format(dtype, set(i.dtype for i in lst))\n",
    "    elif isinstance(lst[0], (int, float, complex, np.number)):\n",
    "        return np.array(lst, dtype=dtype)\n",
    "    else:\n",
    "        dim1 = list2nparray(lst[0])\n",
    "        if dtype is None:\n",
    "            dtype = dim1.dtype\n",
    "        dim1 = dim1.shape\n",
    "    shape = [len(lst)] + list(dim1)\n",
    "    rst = np.empty(shape, dtype=dtype)\n",
    "    for idx, i in enumerate(lst):\n",
    "        rst[idx] = i\n",
    "    return rst\n",
    "\n",
    "\n",
    "def get_img_list(path):\n",
    "    return sorted(list(Path(path).glob('*.png'))) + \\\n",
    "        sorted(list(Path(path).glob('*.jpg'))) + \\\n",
    "        sorted(list(Path(path).glob('*.jpeg')))\n",
    "\n",
    "def gen_miss(img, mask, output):\n",
    "    imgs = get_img_list(img)\n",
    "    masks = get_img_list(mask)\n",
    "    print('Total images:', len(imgs), len(masks))\n",
    "\n",
    "    out = Path(output)\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for i, (img, mask) in tqdm.tqdm(enumerate(zip(imgs, masks))):\n",
    "        path = out.joinpath('miss_%04d.png' % (i+1))\n",
    "        img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
    "        mask = cv2.imread(str(mask), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = cv2.resize(mask, img.shape[:2][::-1])\n",
    "        mask = mask[..., np.newaxis]\n",
    "        miss = img * (mask > 127) + 255 * (mask <= 127)\n",
    "\n",
    "        cv2.imwrite(str(path), miss)\n",
    "\n",
    "def merge_imgs(dirs, output, row=1, gap=2, res=512):\n",
    "\n",
    "    image_list = [get_img_list(path) for path in dirs]\n",
    "    img_count = [len(image) for image in image_list]\n",
    "    print('Total images:', img_count) # should be batch size when we walk back into here ... \n",
    "    assert min(img_count) > 0, 'Please check the path of empty folder.'\n",
    "\n",
    "    output_dir = Path(output)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    n_img = len(dirs)\n",
    "    row = row\n",
    "    column = (n_img - 1) // row + 1\n",
    "    print('Row:', row)\n",
    "    print('Column:', column)\n",
    "\n",
    "    for i, unit in tqdm.tqdm(enumerate(zip(*image_list))):\n",
    "        name = output_dir.joinpath('merge_%04d.png' % i)\n",
    "        merge = np.ones([\n",
    "            res*row + (row+1)*gap, res*column + (column+1)*gap, 3], np.uint8) * 255\n",
    "        for j, img in enumerate(unit):\n",
    "            r = j // column\n",
    "            c = j - r * column\n",
    "            img = cv2.imread(str(img), cv2.IMREAD_COLOR)\n",
    "            if img.shape[:2] != (res, res):\n",
    "                img = cv2.resize(img, (res, res))\n",
    "            start_h, start_w = (r + 1) * gap + r * res, (c + 1) * gap + c * res\n",
    "            merge[start_h: start_h + res, start_w: start_w + res] = img\n",
    "        cv2.imwrite(str(name), merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF-Net Model\n",
    "\n",
    "def get_norm(name, out_channels):\n",
    "    if name == 'batch':\n",
    "        norm = nn.BatchNorm2d(out_channels)\n",
    "    elif name == 'instance':\n",
    "        norm = nn.InstanceNorm2d(out_channels)\n",
    "    else:\n",
    "        norm = None\n",
    "    return norm\n",
    "\n",
    "\n",
    "def get_activation(name):\n",
    "    if name == 'relu':\n",
    "        activation = nn.ReLU()\n",
    "    elif name == 'elu':\n",
    "        activation == nn.ELU()\n",
    "    elif name == 'leaky_relu':\n",
    "        activation = nn.LeakyReLU(negative_slope=0.2)\n",
    "    elif name == 'tanh':\n",
    "        activation = nn.Tanh()\n",
    "    elif name == 'sigmoid':\n",
    "        activation = nn.Sigmoid()\n",
    "    else:\n",
    "        activation = None\n",
    "    return activation\n",
    "\n",
    "\n",
    "class Conv2dSame(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        padding = self.conv_same_pad(kernel_size, stride)\n",
    "        if type(padding) is not tuple:\n",
    "            self.conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size, stride, padding)\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.ConstantPad2d(padding*2, 0),\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size, stride, 0)\n",
    "            )\n",
    "\n",
    "    def conv_same_pad(self, ksize, stride):\n",
    "        if (ksize - stride) % 2 == 0:\n",
    "            return (ksize - stride) // 2\n",
    "        else:\n",
    "            left = (ksize - stride) // 2\n",
    "            right = left + 1\n",
    "            return left, right\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ConvTranspose2dSame(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        padding, output_padding = self.deconv_same_pad(kernel_size, stride)\n",
    "        self.trans_conv = nn.ConvTranspose2d(\n",
    "            in_channels, out_channels, kernel_size, stride,\n",
    "            padding, output_padding)\n",
    "\n",
    "    def deconv_same_pad(self, ksize, stride):\n",
    "        pad = (ksize - stride + 1) // 2\n",
    "        outpad = 2 * pad + stride - ksize\n",
    "        return pad, outpad\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.trans_conv(x)\n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, mode='nearest', scale=2, channel=None, kernel_size=4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mode = mode\n",
    "        if mode == 'deconv':\n",
    "            self.up = ConvTranspose2dSame(\n",
    "                channel, channel, kernel_size, stride=scale)\n",
    "        else:\n",
    "            def upsample(x):\n",
    "                return F.interpolate(x, scale_factor=scale, mode=mode)\n",
    "            self.up = upsample\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(x)\n",
    "\n",
    "\n",
    "class EncodeBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, in_channels, out_channels, kernel_size, stride,\n",
    "            normalization=None, activation=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_in = in_channels\n",
    "        self.c_out = out_channels\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            Conv2dSame(self.c_in, self.c_out, kernel_size, stride))\n",
    "        if normalization:\n",
    "            layers.append(get_norm(normalization, self.c_out))\n",
    "        if activation:\n",
    "            layers.append(get_activation(activation))\n",
    "        self.encode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "\n",
    "\n",
    "class DecodeBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, c_from_up, c_from_down, c_out, mode='nearest',\n",
    "            kernel_size=4, scale=2, normalization='batch', activation='relu'):\n",
    "        super().__init__()\n",
    "\n",
    "        self.c_from_up = c_from_up\n",
    "        self.c_from_down = c_from_down\n",
    "        self.c_in = c_from_up + c_from_down\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.up = UpBlock(mode, scale, c_from_up, kernel_size=scale)\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            Conv2dSame(self.c_in, self.c_out, kernel_size, stride=1))\n",
    "        if normalization:\n",
    "            layers.append(get_norm(normalization, self.c_out))\n",
    "        if activation:\n",
    "            layers.append(get_activation(activation))\n",
    "        self.decode = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, concat=None):\n",
    "        out = self.up(x)\n",
    "        if self.c_from_down > 0:\n",
    "            out = torch.cat([out, concat], dim=1)\n",
    "        out = self.decode(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BlendBlock(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self, c_in, c_out, ksize_mid=3, norm='batch', act='leaky_relu'):\n",
    "        super().__init__()\n",
    "        c_mid = max(c_in // 2, 32)\n",
    "        self.blend = nn.Sequential(\n",
    "            Conv2dSame(c_in, c_mid, 1, 1),\n",
    "            get_norm(norm, c_mid),\n",
    "            get_activation(act),\n",
    "            Conv2dSame(c_mid, c_out, ksize_mid, 1),\n",
    "            get_norm(norm, c_out),\n",
    "            get_activation(act),\n",
    "            Conv2dSame(c_out, c_out, 1, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blend(x)\n",
    "\n",
    "\n",
    "class FusionBlock(nn.Module):\n",
    "    def __init__(self, c_feat, c_alpha=1):\n",
    "        super().__init__()\n",
    "        c_img = 3\n",
    "        self.map2img = nn.Sequential(\n",
    "            Conv2dSame(c_feat, c_img, 1, 1),\n",
    "            nn.Sigmoid())\n",
    "        self.blend = BlendBlock(c_img*2, c_alpha)\n",
    "\n",
    "    def forward(self, img_miss, feat_de):\n",
    "        img_miss = resize_like(img_miss, feat_de)\n",
    "        raw = self.map2img(feat_de)\n",
    "        alpha = self.blend(torch.cat([img_miss, raw], dim=1))\n",
    "        result = alpha * raw + (1 - alpha) * img_miss\n",
    "        return result, alpha, raw\n",
    "\n",
    "\n",
    "class DFNet(nn.Module):\n",
    "    def __init__(\n",
    "            self, c_img=3, c_mask=1, c_alpha=3,\n",
    "            mode='nearest', norm='batch', act_en='relu', act_de='leaky_relu',\n",
    "            en_ksize=[7, 5, 5, 3, 3, 3, 3, 3], de_ksize=[3]*8,\n",
    "            blend_layers=[0, 1, 2, 3, 4, 5]):\n",
    "        super().__init__()\n",
    "\n",
    "        c_init = c_img + c_mask\n",
    "\n",
    "        self.n_en = len(en_ksize)\n",
    "        self.n_de = len(de_ksize)\n",
    "        assert self.n_en == self.n_de, (\n",
    "            'The number layer of Encoder and Decoder must be equal.')\n",
    "        assert self.n_en >= 1, (\n",
    "            'The number layer of Encoder and Decoder must be greater than 1.')\n",
    "\n",
    "        assert 0 in blend_layers, 'Layer 0 must be blended.'\n",
    "\n",
    "        self.en = []\n",
    "        c_in = c_init\n",
    "        self.en.append(\n",
    "            EncodeBlock(c_in, 64, en_ksize[0], 2, None, None))\n",
    "        for k_en in en_ksize[1:]:\n",
    "            c_in = self.en[-1].c_out\n",
    "            c_out = min(c_in*2, 512)\n",
    "            self.en.append(EncodeBlock(\n",
    "                c_in, c_out, k_en, stride=2,\n",
    "                normalization=norm, activation=act_en))\n",
    "\n",
    "        # register parameters\n",
    "        for i, en in enumerate(self.en):\n",
    "            self.__setattr__('en_{}'.format(i), en)\n",
    "\n",
    "        self.de = []\n",
    "        self.fuse = []\n",
    "        for i, k_de in enumerate(de_ksize):\n",
    "\n",
    "            c_from_up = self.en[-1].c_out if i == 0 else self.de[-1].c_out\n",
    "            c_out = c_from_down = self.en[-i-1].c_in\n",
    "            layer_idx = self.n_de - i - 1\n",
    "\n",
    "            self.de.append(DecodeBlock(\n",
    "                c_from_up, c_from_down, c_out, mode, k_de, scale=2,\n",
    "                normalization=norm, activation=act_de))\n",
    "            if layer_idx in blend_layers:\n",
    "                self.fuse.append(FusionBlock(c_out, c_alpha))\n",
    "            else:\n",
    "                self.fuse.append(None)\n",
    "\n",
    "        # register parameters\n",
    "        for i, de in enumerate(self.de[::-1]):\n",
    "            self.__setattr__('de_{}'.format(i), de)\n",
    "        for i, fuse in enumerate(self.fuse[::-1]):\n",
    "            if fuse:\n",
    "                self.__setattr__('fuse_{}'.format(i), fuse)\n",
    "\n",
    "    def forward(self, img_miss, mask):\n",
    "\n",
    "        out = torch.cat([img_miss, mask], dim=1)\n",
    "\n",
    "        out_en = [out]\n",
    "        for encode in self.en:\n",
    "            out = encode(out)\n",
    "            out_en.append(out)\n",
    "\n",
    "        results = []\n",
    "        alphas = []\n",
    "        raws = []\n",
    "        for i, (decode, fuse) in enumerate(zip(self.de, self.fuse)):\n",
    "            out = decode(out, out_en[-i-2])\n",
    "            if fuse:\n",
    "                result, alpha, raw = fuse(img_miss, out)\n",
    "                results.append(result)\n",
    "                alphas.append(alpha)\n",
    "                raws.append(raw)\n",
    "\n",
    "        return results[::-1], alphas[::-1], raws[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inpainter \n",
    "\n",
    "class Inpainter:\n",
    "\n",
    "    def __init__(self, model_path, input_size, batch_size):\n",
    "        self.model_path = model_path\n",
    "        self._input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "        self.init_model(model_path)\n",
    "\n",
    "    @property\n",
    "    def input_size(self):\n",
    "        if self._input_size > 0:\n",
    "            return (self._input_size, self._input_size)\n",
    "        elif 'celeba' in self.model_path:\n",
    "            return (256, 256)\n",
    "        else:\n",
    "            return (256, 256)\n",
    "\n",
    "    def init_model(self, path):\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "            print('Using gpu.')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "            print('Using cpu.')\n",
    "\n",
    "        self.model = DFNet().to(self.device)\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.model.load_state_dict(checkpoint)\n",
    "        self.model.eval()\n",
    "\n",
    "        print('Model %s loaded.' % path)\n",
    "\n",
    "    def get_name(self, path):\n",
    "        return '.'.join(path.name.split('.')[:-1])\n",
    "\n",
    "    def results_path(self, output, img_path, mask_path, prefix='result'):\n",
    "        img_name = self.get_name(img_path)\n",
    "        mask_name = self.get_name(mask_path)\n",
    "        return {\n",
    "            'result_path': self.sub_dir('result').joinpath(\n",
    "                'result-{}-{}.png'.format(img_name, mask_name)),\n",
    "            'raw_path': self.sub_dir('raw').joinpath(\n",
    "                'raw-{}-{}.png'.format(img_name, mask_name)),\n",
    "            'alpha_path': self.sub_dir('alpha').joinpath(\n",
    "                'alpha-{}-{}.png'.format(img_name, mask_name))\n",
    "        }\n",
    "\n",
    "    def inpaint_instance(self, img, mask):\n",
    "        \"\"\"Assume color image with 3 dimension. CWH\"\"\"\n",
    "        img = img.view(1, *img.shape)\n",
    "        mask = mask.view(1, 1, *mask.shape)\n",
    "        return self.inpaint_batch(img, mask).squeeze()\n",
    "\n",
    "    def inpaint_batch(self, imgs, masks):\n",
    "        \"\"\"Assume color channel is BGR and input is NWHC np.uint8.\"\"\"\n",
    "        imgs = np.transpose(imgs, [0, 3, 1, 2])\n",
    "        masks = np.transpose(masks, [0, 3, 1, 2])\n",
    "\n",
    "        imgs = torch.from_numpy(imgs).to(self.device)\n",
    "        masks = torch.from_numpy(masks).to(self.device)\n",
    "        imgs = imgs.float().div(255)\n",
    "        masks = masks.float().div(255)\n",
    "        imgs_miss = imgs * masks\n",
    "        results = self.model(imgs_miss, masks)\n",
    "        if type(results) is list:\n",
    "            results = results[0]\n",
    "        results = results.mul(255).byte().data.cpu().numpy()\n",
    "        results = np.transpose(results, [0, 2, 3, 1])\n",
    "        return results\n",
    "\n",
    "    def _process_file(self, output, img_path, mask_path):\n",
    "        item = {\n",
    "            'img_path': img_path,\n",
    "            'mask_path': mask_path,\n",
    "        }\n",
    "        item.update(self.results_path(output, img_path, mask_path))\n",
    "        self.path_pair.append(item)\n",
    "\n",
    "    def process_single_file(self, output, img_path, mask_path):\n",
    "        self.path_pair = []\n",
    "        self._process_file(output, img_path, mask_path)\n",
    "\n",
    "    def process_dir(self, output, img_dir, mask_dir):\n",
    "        img_dir = Path(img_dir)\n",
    "        mask_dir = Path(mask_dir)\n",
    "        imgs_path = sorted(\n",
    "            list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png')))\n",
    "        masks_path = sorted(\n",
    "            list(mask_dir.glob('*.jpg')) + list(mask_dir.glob('*.png')))\n",
    "\n",
    "        n_img = len(imgs_path)\n",
    "        n_mask = len(masks_path)\n",
    "        n_pair = min(n_img, n_mask)\n",
    "\n",
    "        self.path_pair = []\n",
    "        for i in range(n_pair):\n",
    "            img_path = imgs_path[i % n_img]\n",
    "            mask_path = masks_path[i % n_mask]\n",
    "            self._process_file(output, img_path, mask_path)\n",
    "\n",
    "    def get_process(self, input_size):\n",
    "        def process(pair):\n",
    "            img = cv2.imread(str(pair['img_path']), cv2.IMREAD_COLOR)\n",
    "            mask = cv2.imread(str(pair['mask_path']), cv2.IMREAD_GRAYSCALE)\n",
    "            if input_size:\n",
    "#                 img = img[0:256, 0:256]\n",
    "#                 mask = mask[0:64, 0:64] ## ALSO HERE\n",
    "                img = cv2.resize(img, input_size)\n",
    "                mask = cv2.resize(mask, input_size)\n",
    "            img = np.ascontiguousarray(img.transpose(2, 0, 1)).astype(np.uint8)\n",
    "            mask = np.ascontiguousarray(\n",
    "                np.expand_dims(mask, 0)).astype(np.uint8)\n",
    "\n",
    "            pair['img'] = img\n",
    "            pair['mask'] = mask\n",
    "            return pair\n",
    "        return process\n",
    "\n",
    "    def _file_batch(self):\n",
    "        pool = Pool() # should be os.cpu_count() ... \n",
    "\n",
    "        n_pair = len(self.path_pair)\n",
    "        n_batch = (n_pair-1) // self.batch_size + 1\n",
    "        \n",
    "##        n_batch = 4\n",
    "        \n",
    "        for i in tqdm.trange(n_batch, leave=False):            \n",
    "            _buffer = defaultdict(list)\n",
    "            print(_buffer)\n",
    "            start = i * self.batch_size\n",
    "            stop = start + self.batch_size\n",
    "            process = self.get_process(self.input_size)\n",
    "            batch = pool.imap_unordered(\n",
    "                process, islice(self.path_pair, start, stop))\n",
    "            # so we aren't going through here, EDIT now we are  \n",
    "            \n",
    "            for instance in batch:\n",
    "                for k, v in instance.items():\n",
    "                    _buffer[k].append(v)\n",
    "            yield _buffer\n",
    "            \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    def batch_generator(self):\n",
    "        generator = self._file_batch\n",
    "        \n",
    "        for _buffer in generator():\n",
    "            for key in _buffer:\n",
    "                \n",
    "                \n",
    "                if key in ['img', 'mask']:\n",
    "                    _buffer[key] = list2nparray(_buffer[key])\n",
    "            yield _buffer\n",
    "\n",
    "    def to_numpy(self, tensor):\n",
    "        tensor = tensor.mul(255).byte().data.cpu().numpy()\n",
    "        tensor = np.transpose(tensor, [0, 2, 3, 1])\n",
    "        return tensor\n",
    "\n",
    "    def process_batch(self, batch, output, results):\n",
    "        imgs = torch.from_numpy(batch['img']).to(self.device)\n",
    "        masks = torch.from_numpy(batch['mask']).to(self.device)\n",
    "        imgs = imgs.float().div(255)\n",
    "        masks = masks.float().div(255)\n",
    "        imgs_miss = imgs * masks\n",
    "\n",
    "        result, alpha, raw = self.model(imgs_miss, masks)\n",
    "        result, alpha, raw = result[0], alpha[0], raw[0]\n",
    "        result = imgs * masks + result * (1 - masks)\n",
    "        \n",
    "        result = self.to_numpy(result)\n",
    "        \n",
    "#         results.append(result)\n",
    "        results = np.append(results, result, axis=0)\n",
    "\n",
    "    \n",
    "        alpha = self.to_numpy(alpha)\n",
    "        raw = self.to_numpy(raw)\n",
    "            \n",
    "        for i in range(result.shape[0]):\n",
    "            cv2.imwrite(str(batch['result_path'][i]), result[i])\n",
    "            cv2.imwrite(str(batch['raw_path'][i]), raw[i])\n",
    "            cv2.imwrite(str(batch['alpha_path'][i]), alpha[i])\n",
    "                    \n",
    "    @property\n",
    "    def root(self):\n",
    "        return Path(self.output)\n",
    "\n",
    "    def sub_dir(self, sub):\n",
    "        return self.root.joinpath(sub)\n",
    "\n",
    "    def prepare_folders(self, folders):\n",
    "        for folder in folders:\n",
    "            Path(folder).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def inpaint(self, output, img, mask, merge_result=False):\n",
    "\n",
    "        self.output = output\n",
    "        self.prepare_folders([\n",
    "            self.sub_dir('result'), self.sub_dir('alpha'),\n",
    "            self.sub_dir('raw')])\n",
    "\n",
    "        if os.path.isfile(img) and os.path.isfile(mask):\n",
    "            if img.endswith(('.png', '.jpg', '.jpeg')):\n",
    "                self.process_single_file(output, img, mask)\n",
    "                _type = 'file'\n",
    "            else:\n",
    "                raise NotImplementedError()\n",
    "        elif os.path.isdir(img) and os.path.isdir(mask):\n",
    "            self.process_dir(output, img, mask)\n",
    "            _type = 'dir'\n",
    "        else:\n",
    "            print('Img: ', img)\n",
    "            print('Mask: ', mask)\n",
    "            raise NotImplementedError(\n",
    "                'img and mask should be both file or directory.')\n",
    "\n",
    "        print('# Inpainting...')\n",
    "        print('Input size:', self.input_size)\n",
    "        results = np.empty((32,256,256,3), int) # it's just following this dimension? # are we getting the error here?\n",
    "#         results = []\n",
    "        for batch in self.batch_generator():\n",
    "            self.process_batch(batch, output, results)\n",
    "        print('Inpainting finished.')\n",
    "        ## so results should contain everything now \n",
    "\n",
    "        if merge_result and _type == 'dir':\n",
    "            miss = self.sub_dir('miss')\n",
    "            merge = self.sub_dir('merge')\n",
    "\n",
    "            print('# Preparing input images...')\n",
    "            gen_miss(img, mask, miss) # dfnet util fn\n",
    "            print('# Merging...')\n",
    "            merge_imgs([\n",
    "                miss, self.sub_dir('raw'), self.sub_dir('alpha'),\n",
    "                self.sub_dir('result'), img], merge, res=self.input_size[0])\n",
    "            print('Merging finished.')\n",
    "            \n",
    "            \n",
    "        \n",
    "        results = np.transpose(results, [0, 3, 1, 2])\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Trainer\n",
    "def train_model():\n",
    "    \"\"\"Generic function to train model\"\"\"\n",
    "\n",
    "    start_time = datetime.now()\n",
    "    correct = 0 \n",
    "    incorrect = 0 \n",
    "    num_batches = 1\n",
    "    loss_values = []\n",
    "\n",
    "    # DF-Net Tester Instantiate\n",
    "    pretrained_model_path = './model/model_places2.pth'\n",
    "    inpainter = Inpainter(pretrained_model_path, 256, 8) # was 8, should it be 1 or 32 or 128?\n",
    "\n",
    "    # Epochs \n",
    "    for epoch in range(num_epochs): \n",
    "        print(\"epoch num:\", epoch)\n",
    "        \n",
    "        running_outputs = torch.FloatTensor().cpu()\n",
    "        running_labels = torch.LongTensor().cpu()\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "#         model.eval()\n",
    "        # Batches\n",
    "        for batch_idx, (inputs, labels) in enumerate(train_loader): \n",
    "            \n",
    "            img_path = './samples/places2/img/'\n",
    "            os.makedirs(img_path)\n",
    "            \n",
    "            # eventually crashes 8 for some reason \n",
    "            print(\"batch_idx\", batch_idx)\n",
    "            \n",
    "            for img_idx in range(batch_size-1):\n",
    "                \n",
    "                img = inputs[img_idx] # or is it running out here\n",
    "                \n",
    "                img_idx_name = ''\n",
    "                # so that should be 0-9\n",
    "                if img_idx < 10:\n",
    "                    img_idx_name = '0%d' % img_idx\n",
    "                else: \n",
    "                    img_idx_name = '%d' % img_idx\n",
    "\n",
    "                save_image(img, './samples/places2/img/img_{}.png'.format(img_idx_name))\n",
    "            \n",
    "            inpainted_img_batch = inpainter.inpaint('output/places2/', img_path, mask_path, merge_result=True)\n",
    "            inpainted_img_batch = torch.from_numpy(inpainted_img_batch)\n",
    "#             print(type(inpainted_img_batch[0]))\n",
    "#             print(inpainted_img_batch[0])\n",
    "            \n",
    "            # delete img_path\n",
    "            shutil.rmtree(img_path)\n",
    "                            \n",
    "            optimizer.zero_grad()\n",
    "            inpainted_img_batch, labels = inpainted_img_batch.to(device, dtype=torch.float), labels.to(device)\n",
    "            output = model.forward(inpainted_img_batch)\n",
    "            \n",
    "#             print(\"inpainted_img_batch.shape\",inpainted_img_batch.shape)\n",
    "#             print(\"labels.shape\",labels.shape)\n",
    "            \n",
    "            loss = criterion(output, labels)\n",
    "            loss = Variable(loss, requires_grad = True)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "            running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "        # Accuracy\n",
    "        for idx, emb in enumerate(running_outputs):\n",
    "            print(\"computing accuracy ...\")\n",
    "            pairwise = torch.nn.PairwiseDistance(p=2).to(device)\n",
    "            dist = pairwise(emb, running_outputs)\n",
    "            closest = torch.topk(dist, 2, largest=False).indices[1]\n",
    "            if running_labels[idx] == running_labels[closest]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "\n",
    "        print(running_loss / num_batches)\n",
    "        print(\"Correct\", correct)\n",
    "        print(\"Incorrect\", incorrect)\n",
    "\n",
    "        # t-SNE\n",
    "        map_features(running_outputs, running_labels, \"outfile\")\n",
    "        # Loss Plot            \n",
    "        loss_values.append(running_loss / num_batches)\n",
    "\n",
    "        time_elapsed = datetime.now() - start_time \n",
    "        print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))    \n",
    "\n",
    "    plt.plot(loss_values)    \n",
    "    return model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using gpu.\n",
      "Model ./model/model_places2.pth loaded.\n",
      "epoch num: 0\n",
      "batch_idx 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Inpainting...\n",
      "Input size: (256, 256)\n",
      "defaultdict(<class 'list'>, {})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:00, 44.71it/s]                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inpainting finished.\n",
      "# Preparing input images...\n",
      "Total images: 511 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 48.06it/s]\n",
      "2it [00:00, 16.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Merging...\n",
      "Total images: [32, 83, 83, 83, 511]\n",
      "Row: 1\n",
      "Column: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:01, 16.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging finished.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Number of embeddings must equal number of labels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-8f26edc6747e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# perhaps parameterize the train model to take in our pre-processed data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-9a031c924438>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print(\"labels.shape\",labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pytorch_metric_learning/losses/base_metric_loss_function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels, indices_tuple)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \"\"\"\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mc_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_embeddings_and_labels_are_same_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pytorch_metric_learning/utils/common_functions.py\u001b[0m in \u001b[0;36massert_embeddings_and_labels_are_same_size\u001b[0;34m(embeddings, labels)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0massert_embeddings_and_labels_are_same_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Number of embeddings must equal number of labels\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Number of embeddings must equal number of labels"
     ]
    }
   ],
   "source": [
    "# Run Script\n",
    "model.to(device)\n",
    "\n",
    "# perhaps parameterize the train model to take in our pre-processed data\n",
    "\n",
    "trained_model, loss = train_model()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: Expected 4-dimensional input for 4-dimensional weight 64 3 7 7,\n",
    "# but got 5-dimensional input of size [4, 8, 256, 256, 3] instead\n",
    "# so just reshape that np first. change torch channel placement\n",
    "\n",
    "\n",
    "## inpainted_img_batch.shape torch.Size([8, 3, 256, 256])\n",
    "## labels.shape torch.Size([32])\n",
    "\n",
    "# Debug cell\n",
    "train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "                                       sampler=train_sampler, num_workers=4)\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader): \n",
    "    print(batch_idx)\n",
    "    print(type(inputs))\n",
    "    # plot\n",
    "#     plt.imshow(transforms.ToPILImage()(inputs[31]), interpolation=\"bicubic\")\n",
    "    print(inputs.shape)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Script\n",
    "\n",
    "print(type(trained_model))\n",
    "trained_model.test()\n",
    "\n",
    "print(\"Finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "def test():\n",
    "    trained_model.eval()\n",
    "    test_running_outputs = torch.FloatTensor().cpu()\n",
    "    test_running_labels = torch.LongTensor().cpu()\n",
    "    test_running_loss = 0.0    \n",
    "    correct = 0 \n",
    "    incorrect = 0 \n",
    "    \n",
    "    test_sampler = torch.utils.data.RandomSampler(test_set)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \n",
    "                                           sampler=test_sampler, num_workers=4)\n",
    "    \n",
    "    with torch.no_grad():        \n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            output = trained_model(data)\n",
    "            \n",
    "            test_running_outputs = torch.cat((test_running_outputs, output.cpu().detach()), 0)\n",
    "            test_running_labels = torch.cat((test_running_labels, labels.cpu().detach()), 0)\n",
    "    \n",
    "            test_loss = criterion(output, labels)\n",
    "            test_running_loss += loss.item()\n",
    "\n",
    "        # Accuracy\n",
    "        for idx, emb in enumerate(running_outputs.to(device)):    \n",
    "            pairwise = torch.nn.PairwiseDistance(p=2).to(device)\n",
    "            dist = pairwise(emb, running_outputs.to(device))\n",
    "            closest = torch.topk(dist, 2, largest=False).indices[1]\n",
    "            if running_labels[idx] == running_labels[closest]:\n",
    "                correct += 1\n",
    "            else:\n",
    "                incorrect += 1\n",
    "                \n",
    "        map_features(test_running_outputs, test_running_labels, \"test_outfile\")        \n",
    "        print(\"correct\", correct)\n",
    "        print(\"incorrect\", incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DF-Net Train Loop \n",
    "# def train_dfnet():\n",
    "#     \"\"\"Generic function to train model\"\"\"\n",
    "    \n",
    "#     print(\"Training model ...\")\n",
    "\n",
    "#     start_time = datetime.now()\n",
    "#     loss_values = []\n",
    "#     num_batches = 0 \n",
    "    \n",
    "#     # Epochs \n",
    "#     for epoch in range(num_epochs): \n",
    "#         print(\"epoch num:\", epoch)\n",
    "#         train_sampler = torch.utils.data.RandomSampler(train_set)\n",
    "#         train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, \n",
    "#                                                sampler=train_sampler, num_workers=4)\n",
    "       \n",
    "#         running_outputs = torch.FloatTensor().cpu()\n",
    "#         running_labels = torch.LongTensor().cpu()\n",
    "#         running_loss = 0.0\n",
    "#         dfnet_model.train()\n",
    "        \n",
    "#         # Batches\n",
    "#         for batch_idx, (inputs, labels) in enumerate(train_loader):  \n",
    "                        \n",
    "#             num_batches += 1\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "#             output = dfnet_model.forward(inputs)\n",
    "            \n",
    "#             running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "#             running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "\n",
    "#             loss = criterion(output, labels)\n",
    "#             loss = Variable(loss, requires_grad = True)\n",
    "            \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "            \n",
    "#             running_loss += loss.item()\n",
    "\n",
    "#         # Loss Plot            \n",
    "#         loss_values.append(running_loss / num_batches)\n",
    "\n",
    "#         time_elapsed = datetime.now() - start_time \n",
    "#         print('Time elapsed (hh:mm:ss.ms) {}'.format(time_elapsed))    \n",
    "\n",
    "#     plt.plot(loss_values)    \n",
    "#     return dfnet_model, running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DF-Net outputs\n",
    "# # TODO: implement this\n",
    "# # but how can we parameterize thi?\n",
    "\n",
    "# def inpaint(): \n",
    "#     running_outputs = torch.FloatTensor().cpu()\n",
    "#     running_labels = torch.LongTensor().cpu()\n",
    "#     running_loss = 0.0  \n",
    "    \n",
    "#     dfnet_sampler = torch.utils.data.RandomSampler(test_set)\n",
    "#     dfnet_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, \n",
    "#                                            sampler=dfnet_sampler, num_workers=4)\n",
    "    \n",
    "#     with torch.no_grad():        \n",
    "#         for data, labels in dfnet_loader:\n",
    "#             data, labels = data.to(device), labels.to(device)\n",
    "#             output = model(data)\n",
    "            \n",
    "#             running_outputs = torch.cat((running_outputs, output.cpu().detach()), 0)\n",
    "#             running_labels = torch.cat((running_labels, labels.cpu().detach()), 0)\n",
    "    \n",
    "#             loss = criterion(output, labels)\n",
    "#             running_loss += loss.item()\n",
    "\n",
    "            \n",
    "    \n",
    "#     print(type(output))\n",
    "#     print(output.shape)\n",
    "#     print(type(output[0][0]))\n",
    "    \n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train DF-Net\n",
    "# dfnet_model.to(device)\n",
    "# dfnet_model, loss = train_dfnet()\n",
    "\n",
    "# print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
